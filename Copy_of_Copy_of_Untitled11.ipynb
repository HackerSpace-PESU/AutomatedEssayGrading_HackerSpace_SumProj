{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFrO46WcBjqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b58e5849-c55f-4577-d018-bf1cef4124c2"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU7EToMLFxtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y5oYDM06MSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten,BatchNormalization\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import cohen_kappa_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smoe5RqvCF_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"training_set_rel3.tsv\", sep='\\t', encoding = \"ISO-8859-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CPb9of2G9eh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "e369e150-c289-4bd0-e4ab-09e2164d96a3"
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>rater1_domain1</th>\n",
              "      <th>rater2_domain1</th>\n",
              "      <th>rater3_domain1</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>rater1_domain2</th>\n",
              "      <th>rater2_domain2</th>\n",
              "      <th>domain2_score</th>\n",
              "      <th>rater1_trait1</th>\n",
              "      <th>rater1_trait2</th>\n",
              "      <th>rater1_trait3</th>\n",
              "      <th>rater1_trait4</th>\n",
              "      <th>rater1_trait5</th>\n",
              "      <th>rater1_trait6</th>\n",
              "      <th>rater2_trait1</th>\n",
              "      <th>rater2_trait2</th>\n",
              "      <th>rater2_trait3</th>\n",
              "      <th>rater2_trait4</th>\n",
              "      <th>rater2_trait5</th>\n",
              "      <th>rater2_trait6</th>\n",
              "      <th>rater3_trait1</th>\n",
              "      <th>rater3_trait2</th>\n",
              "      <th>rater3_trait3</th>\n",
              "      <th>rater3_trait4</th>\n",
              "      <th>rater3_trait5</th>\n",
              "      <th>rater3_trait6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  essay_set  ... rater3_trait5  rater3_trait6\n",
              "0         1          1  ...           NaN            NaN\n",
              "1         2          1  ...           NaN            NaN\n",
              "\n",
              "[2 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u0wLysaHF-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col_to_keep =['essay','domain1_score']\n",
        "filter_data = data[col_to_keep]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozzdQuq3-DA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJcsDCMgHPIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f308f41b-b2a4-4e1e-ef90-1e4415337c36"
      },
      "source": [
        "def word_counting(x):\n",
        "    return (len(TextBlob(x).words))\n",
        "filter_data['word_length'] = filter_data['essay'].apply(word_counting)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zibEdcobcSfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "39f1e3ad-0665-4567-d503-1cc58c2ecc2d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwJIkZdoHV_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6995cef4-7cfc-49f0-fa2c-2284d927d12f"
      },
      "source": [
        "def sentence_counting(x):\n",
        "    sentence_len = len([len(sentence.split(' ')) for sentence in TextBlob(x).sentences])\n",
        "    return sentence_len\n",
        "filter_data['no_of_sentence'] = filter_data['essay'].apply(sentence_counting)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMoo1cdvHatw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "80ead25e-de0b-4727-baa1-27eb203d4555"
      },
      "source": [
        "def avg_sentence_sentiment(x):\n",
        "    sentiment_essay = TextBlob(x).sentiment.polarity\n",
        "    return sentiment_essay\n",
        "filter_data['sentiment_essay'] = filter_data['essay'].apply(avg_sentence_sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLhF0jtaHgYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "57343057-3fd0-47cc-9051-ce6e8abb917a"
      },
      "source": [
        "def avg_length_of_words(x):\n",
        "    word_len = [len(word) for word in TextBlob(x).words]\n",
        "    return ( sum(word_len)/len(word_len) )\n",
        "filter_data['avg_word_len']  = filter_data['essay'].apply(avg_length_of_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62H6z48NH9U2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0488ab57-5bb3-4250-fe11-278bd9f16b1e"
      },
      "source": [
        "def grammar_check(x):\n",
        "    tool = language_check.LanguageTool('en-US')\n",
        "    matches = tool.check(x)\n",
        "    return len(matches)\n",
        "filter_data['Grammar_check'] = filter_data['essay'].apply(grammar_check)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a430e5c9bde5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfilter_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Grammar_check'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'essay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a430e5c9bde5>\u001b[0m in \u001b[0;36mgrammar_check\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrammar_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageTool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Grammar_check'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'essay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'language_check' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rr76Zuu7XgP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3ad6c143-6d94-476c-ad90-e82f240ff49e"
      },
      "source": [
        "display(filter_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "      <th>word_length</th>\n",
              "      <th>no_of_sentence</th>\n",
              "      <th>sentiment_essay</th>\n",
              "      <th>avg_word_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>8</td>\n",
              "      <td>343</td>\n",
              "      <td>16</td>\n",
              "      <td>0.310471</td>\n",
              "      <td>4.358601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>9</td>\n",
              "      <td>422</td>\n",
              "      <td>20</td>\n",
              "      <td>0.274000</td>\n",
              "      <td>4.331754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>7</td>\n",
              "      <td>283</td>\n",
              "      <td>14</td>\n",
              "      <td>0.340393</td>\n",
              "      <td>4.356890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>10</td>\n",
              "      <td>527</td>\n",
              "      <td>27</td>\n",
              "      <td>0.266828</td>\n",
              "      <td>4.851992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>8</td>\n",
              "      <td>470</td>\n",
              "      <td>30</td>\n",
              "      <td>0.199684</td>\n",
              "      <td>4.378723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12971</th>\n",
              "      <td>In most stories mothers and daughters are eit...</td>\n",
              "      <td>35</td>\n",
              "      <td>863</td>\n",
              "      <td>27</td>\n",
              "      <td>0.146336</td>\n",
              "      <td>4.018540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12972</th>\n",
              "      <td>I never understood the meaning laughter is th...</td>\n",
              "      <td>32</td>\n",
              "      <td>556</td>\n",
              "      <td>35</td>\n",
              "      <td>0.147192</td>\n",
              "      <td>3.881295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12973</th>\n",
              "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
              "      <td>40</td>\n",
              "      <td>827</td>\n",
              "      <td>41</td>\n",
              "      <td>0.131967</td>\n",
              "      <td>4.355502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12974</th>\n",
              "      <td>Trippin' on fen...</td>\n",
              "      <td>40</td>\n",
              "      <td>576</td>\n",
              "      <td>39</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>4.133681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12975</th>\n",
              "      <td>Many people believe that laughter can improve...</td>\n",
              "      <td>40</td>\n",
              "      <td>475</td>\n",
              "      <td>29</td>\n",
              "      <td>0.247257</td>\n",
              "      <td>4.195789</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12976 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   essay  ...  avg_word_len\n",
              "0      Dear local newspaper, I think effects computer...  ...      4.358601\n",
              "1      Dear @CAPS1 @CAPS2, I believe that using compu...  ...      4.331754\n",
              "2      Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  ...      4.356890\n",
              "3      Dear Local Newspaper, @CAPS1 I have found that...  ...      4.851992\n",
              "4      Dear @LOCATION1, I know having computers has a...  ...      4.378723\n",
              "...                                                  ...  ...           ...\n",
              "12971   In most stories mothers and daughters are eit...  ...      4.018540\n",
              "12972   I never understood the meaning laughter is th...  ...      3.881295\n",
              "12973  When you laugh, is @CAPS5 out of habit, or is ...  ...      4.355502\n",
              "12974                                 Trippin' on fen...  ...      4.133681\n",
              "12975   Many people believe that laughter can improve...  ...      4.195789\n",
              "\n",
              "[12976 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fvZSbwOBviX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=data['domain1_score']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMWu-Et9Bo7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(filter_data, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0-F1nGoEJcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_e = X_train['essay'].tolist()\n",
        "test_e = X_test['essay'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_m1MsdDEcYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sents=[]\n",
        "test_sents=[]\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def sent2word(x):\n",
        "    x=re.sub(\"[^A-Za-z]\",\" \",x)\n",
        "    x.lower()\n",
        "    filtered_sentence = [] \n",
        "    words=x.split()\n",
        "    for w in words:\n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence\n",
        "\n",
        "def essay2word(essay):\n",
        "    essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw = tokenizer.tokenize(essay)\n",
        "    final_words=[]\n",
        "    for i in raw:\n",
        "        if(len(i)>0):\n",
        "            final_words.append(sent2word(i))\n",
        "    return final_words\n",
        "\n",
        "for i in train_e:\n",
        "    train_sents+=essay2word(i)\n",
        "\n",
        "for i in test_e:\n",
        "    test_sents+=essay2word(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oda86E-EAoJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e6be2459-a617-45f2-b339-348e92349e73"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaYan8OqEpjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    BatchNormalization()\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu', kernel_initializer='he_normal'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='RMSProp', metrics=['mae'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAe1Y665Eu0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "93c164d1-0112-4a34-b63c-63a349d06ba6"
      },
      "source": [
        "#Training Word2Vec model\n",
        "num_features = 300\n",
        "min_word_count = 20\n",
        "num_workers = -1\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "model = Word2Vec(train_sents, \n",
        "                 workers=num_workers, \n",
        "                 size=num_features, \n",
        "                 min_count = min_word_count, \n",
        "                 window = context, \n",
        "                 sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH7sMH4CE4XO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "83f06ada-fa20-4fcc-aa05-f0895cf49692"
      },
      "source": [
        "def makeVec(words, model, num_features):\n",
        "    vec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    noOfWords = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for i in words:\n",
        "        if i in index2word_set:\n",
        "            noOfWords += 1\n",
        "            vec = np.add(vec,model[i])        \n",
        "    vec = np.divide(vec,noOfWords)\n",
        "    return vec\n",
        "\n",
        "\n",
        "def getVecs(essays, model, num_features):\n",
        "    c=0\n",
        "    essay_vecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for i in essays:\n",
        "        essay_vecs[c] = makeVec(i, model, num_features)\n",
        "        c+=1\n",
        "    return essay_vecs\n",
        "\n",
        "\n",
        "clean_train=[]\n",
        "for i in train_e:\n",
        "    clean_train.append(sent2word(i))\n",
        "training_vectors = getVecs(clean_train, model, num_features)\n",
        "\n",
        "clean_test=[] \n",
        "\n",
        "for i in test_e:\n",
        "    clean_test.append(sent2word(i))\n",
        "testing_vectors = getVecs(clean_test, model, num_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAXzBIhoKLOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "d0b2cb25-43e3-4010-fd82-ed85d0232b91"
      },
      "source": [
        "!pip install language_check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting language_check\n",
            "  Downloading https://files.pythonhosted.org/packages/97/45/0fd1d3683d6129f30fa09143fa383cdf6dff8bc0d1648f2cf156109cb772/language-check-1.1.tar.gz\n",
            "Building wheels for collected packages: language-check\n",
            "  Building wheel for language-check (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for language-check\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for language-check\n",
            "Failed to build language-check\n",
            "Installing collected packages: language-check\n",
            "    Running setup.py install for language-check ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-sk_tq3fr/language-check/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-sk_tq3fr/language-check/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-648let1y/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQLlj32LKCiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a6056db0-d268-4ae6-e00a-c6fc02f6a452"
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "986wCJncKSMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "eb24c3d4-1264-40a4-e7bb-96985dd558be"
      },
      "source": [
        "\n",
        "training_vectors = np.array(training_vectors)\n",
        "testing_vectors = np.array(testing_vectors)\n",
        "\n",
        "# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "training_vectors = np.reshape(training_vectors, (training_vectors.shape[0], 1, training_vectors.shape[1]))\n",
        "testing_vectors = np.reshape(testing_vectors, (testing_vectors.shape[0], 1, testing_vectors.shape[1]))\n",
        "lstm_model = get_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_15 (LSTM)               (None, 1, 300)            721200    \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 814,705\n",
            "Trainable params: 814,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLHZZCGJKYv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22deefd8-0414-4f2a-e6b0-c72b8685d977"
      },
      "source": [
        "lstm_model.fit(training_vectors, y_train, batch_size=64, epochs=150)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "9083/9083 [==============================] - 2s 246us/step - loss: 11.7338 - mae: 1.8896\n",
            "Epoch 2/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 12.0019 - mae: 1.9238\n",
            "Epoch 3/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 11.6604 - mae: 1.9148\n",
            "Epoch 4/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 11.2400 - mae: 1.8913\n",
            "Epoch 5/150\n",
            "9083/9083 [==============================] - 2s 238us/step - loss: 11.5344 - mae: 1.8933\n",
            "Epoch 6/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 11.5241 - mae: 1.8827\n",
            "Epoch 7/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 11.6067 - mae: 1.8913\n",
            "Epoch 8/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 11.7342 - mae: 1.9164\n",
            "Epoch 9/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 10.8501 - mae: 1.8375\n",
            "Epoch 10/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 11.9491 - mae: 1.8908\n",
            "Epoch 11/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 11.9695 - mae: 1.9147\n",
            "Epoch 12/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 12.0631 - mae: 1.9032\n",
            "Epoch 13/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 11.2072 - mae: 1.8684\n",
            "Epoch 14/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.7164 - mae: 1.8453\n",
            "Epoch 15/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 11.5376 - mae: 1.8807\n",
            "Epoch 16/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.9069 - mae: 1.8336\n",
            "Epoch 17/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.1376 - mae: 1.8134\n",
            "Epoch 18/150\n",
            "9083/9083 [==============================] - 2s 232us/step - loss: 11.1421 - mae: 1.8678\n",
            "Epoch 19/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 11.6625 - mae: 1.8883\n",
            "Epoch 20/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.8560 - mae: 1.8456\n",
            "Epoch 21/150\n",
            "9083/9083 [==============================] - 2s 232us/step - loss: 10.2432 - mae: 1.8159\n",
            "Epoch 22/150\n",
            "9083/9083 [==============================] - 2s 231us/step - loss: 10.6401 - mae: 1.8147\n",
            "Epoch 23/150\n",
            "9083/9083 [==============================] - 2s 233us/step - loss: 10.9568 - mae: 1.8347\n",
            "Epoch 24/150\n",
            "9083/9083 [==============================] - 2s 231us/step - loss: 10.2995 - mae: 1.8020\n",
            "Epoch 25/150\n",
            "9083/9083 [==============================] - 2s 233us/step - loss: 11.7522 - mae: 1.8612\n",
            "Epoch 26/150\n",
            "9083/9083 [==============================] - 2s 232us/step - loss: 10.4615 - mae: 1.8051\n",
            "Epoch 27/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.8385 - mae: 1.8320\n",
            "Epoch 28/150\n",
            "9083/9083 [==============================] - 2s 238us/step - loss: 11.1319 - mae: 1.8480\n",
            "Epoch 29/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 10.4303 - mae: 1.8234\n",
            "Epoch 30/150\n",
            "9083/9083 [==============================] - 2s 238us/step - loss: 11.0645 - mae: 1.8201\n",
            "Epoch 31/150\n",
            "9083/9083 [==============================] - 2s 238us/step - loss: 11.5518 - mae: 1.8598\n",
            "Epoch 32/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.6975 - mae: 1.8191\n",
            "Epoch 33/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.7082 - mae: 1.8159\n",
            "Epoch 34/150\n",
            "9083/9083 [==============================] - 2s 233us/step - loss: 10.9567 - mae: 1.8356\n",
            "Epoch 35/150\n",
            "9083/9083 [==============================] - 2s 232us/step - loss: 10.1937 - mae: 1.7793\n",
            "Epoch 36/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.7521 - mae: 1.7959\n",
            "Epoch 37/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.4927 - mae: 1.7949\n",
            "Epoch 38/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.5857 - mae: 1.7966\n",
            "Epoch 39/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.9526 - mae: 1.8186\n",
            "Epoch 40/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.7821 - mae: 1.8238\n",
            "Epoch 41/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.9558 - mae: 1.8064\n",
            "Epoch 42/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.8354 - mae: 1.8043\n",
            "Epoch 43/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.5955 - mae: 1.8244\n",
            "Epoch 44/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 10.4466 - mae: 1.7914\n",
            "Epoch 45/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.8018 - mae: 1.7933\n",
            "Epoch 46/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 11.1040 - mae: 1.8354\n",
            "Epoch 47/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.0091 - mae: 1.7690\n",
            "Epoch 48/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 10.9037 - mae: 1.8185\n",
            "Epoch 49/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 10.4879 - mae: 1.7757\n",
            "Epoch 50/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 10.2080 - mae: 1.7823\n",
            "Epoch 51/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 10.3963 - mae: 1.7866\n",
            "Epoch 52/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.3713 - mae: 1.7898\n",
            "Epoch 53/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 10.1267 - mae: 1.7813\n",
            "Epoch 54/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.3922 - mae: 1.7727\n",
            "Epoch 55/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 11.1639 - mae: 1.7944\n",
            "Epoch 56/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.9536 - mae: 1.7839\n",
            "Epoch 57/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 10.3275 - mae: 1.7778\n",
            "Epoch 58/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 10.4203 - mae: 1.7672\n",
            "Epoch 59/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 10.1360 - mae: 1.7712\n",
            "Epoch 60/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 10.4247 - mae: 1.7611\n",
            "Epoch 61/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.5485 - mae: 1.7381\n",
            "Epoch 62/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 10.1676 - mae: 1.7559\n",
            "Epoch 63/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.9197 - mae: 1.7833\n",
            "Epoch 64/150\n",
            "9083/9083 [==============================] - 2s 238us/step - loss: 10.1286 - mae: 1.7644\n",
            "Epoch 65/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.9785 - mae: 1.7585\n",
            "Epoch 66/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 9.8466 - mae: 1.7604\n",
            "Epoch 67/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 9.7932 - mae: 1.7504\n",
            "Epoch 68/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 9.9775 - mae: 1.7451\n",
            "Epoch 69/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.3841 - mae: 1.7480\n",
            "Epoch 70/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 9.9313 - mae: 1.7469\n",
            "Epoch 71/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 9.1220 - mae: 1.6993\n",
            "Epoch 72/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 9.4879 - mae: 1.7163\n",
            "Epoch 73/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 10.1912 - mae: 1.7498\n",
            "Epoch 74/150\n",
            "9083/9083 [==============================] - 2s 245us/step - loss: 10.2033 - mae: 1.7365\n",
            "Epoch 75/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 10.3169 - mae: 1.7607\n",
            "Epoch 76/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.8259 - mae: 1.7219\n",
            "Epoch 77/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.6819 - mae: 1.7199\n",
            "Epoch 78/150\n",
            "9083/9083 [==============================] - 2s 233us/step - loss: 9.9844 - mae: 1.7353\n",
            "Epoch 79/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.3742 - mae: 1.7647\n",
            "Epoch 80/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 9.5062 - mae: 1.7203\n",
            "Epoch 81/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.4949 - mae: 1.7123\n",
            "Epoch 82/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 9.7082 - mae: 1.7374\n",
            "Epoch 83/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.2936 - mae: 1.7419\n",
            "Epoch 84/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 10.5302 - mae: 1.7454\n",
            "Epoch 85/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.6906 - mae: 1.7488\n",
            "Epoch 86/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.8509 - mae: 1.6911\n",
            "Epoch 87/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.4237 - mae: 1.7040\n",
            "Epoch 88/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 10.2415 - mae: 1.7303\n",
            "Epoch 89/150\n",
            "9083/9083 [==============================] - 2s 234us/step - loss: 10.0717 - mae: 1.7377\n",
            "Epoch 90/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 10.0102 - mae: 1.7174\n",
            "Epoch 91/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 9.2255 - mae: 1.6954\n",
            "Epoch 92/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.7219 - mae: 1.7218\n",
            "Epoch 93/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 9.6768 - mae: 1.7109\n",
            "Epoch 94/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.5653 - mae: 1.7011\n",
            "Epoch 95/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 9.2693 - mae: 1.7037\n",
            "Epoch 96/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.0414 - mae: 1.6949\n",
            "Epoch 97/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.5448 - mae: 1.7060\n",
            "Epoch 98/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 9.3911 - mae: 1.6974\n",
            "Epoch 99/150\n",
            "9083/9083 [==============================] - 2s 235us/step - loss: 9.6487 - mae: 1.7060\n",
            "Epoch 100/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.2908 - mae: 1.7030\n",
            "Epoch 101/150\n",
            "9083/9083 [==============================] - 2s 236us/step - loss: 9.4858 - mae: 1.7017\n",
            "Epoch 102/150\n",
            "9083/9083 [==============================] - 2s 237us/step - loss: 9.6126 - mae: 1.7041\n",
            "Epoch 103/150\n",
            "9083/9083 [==============================] - 2s 242us/step - loss: 9.9764 - mae: 1.7246\n",
            "Epoch 104/150\n",
            "9083/9083 [==============================] - 2s 246us/step - loss: 9.3930 - mae: 1.6910\n",
            "Epoch 105/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.2606 - mae: 1.6932\n",
            "Epoch 106/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 9.4477 - mae: 1.6962\n",
            "Epoch 107/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.7853 - mae: 1.7066\n",
            "Epoch 108/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 9.7847 - mae: 1.7171\n",
            "Epoch 109/150\n",
            "9083/9083 [==============================] - 2s 246us/step - loss: 9.1360 - mae: 1.6635\n",
            "Epoch 110/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.2488 - mae: 1.6784\n",
            "Epoch 111/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.1061 - mae: 1.6752\n",
            "Epoch 112/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 8.7131 - mae: 1.6787\n",
            "Epoch 113/150\n",
            "9083/9083 [==============================] - 2s 239us/step - loss: 9.1064 - mae: 1.6716\n",
            "Epoch 114/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 9.1887 - mae: 1.6625\n",
            "Epoch 115/150\n",
            "9083/9083 [==============================] - 2s 245us/step - loss: 9.5342 - mae: 1.6954\n",
            "Epoch 116/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 8.8348 - mae: 1.6617\n",
            "Epoch 117/150\n",
            "9083/9083 [==============================] - 2s 249us/step - loss: 8.8672 - mae: 1.6658\n",
            "Epoch 118/150\n",
            "9083/9083 [==============================] - 2s 250us/step - loss: 8.8893 - mae: 1.6710\n",
            "Epoch 119/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 8.6593 - mae: 1.6558\n",
            "Epoch 120/150\n",
            "9083/9083 [==============================] - 2s 250us/step - loss: 9.0666 - mae: 1.6757\n",
            "Epoch 121/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.0601 - mae: 1.6688\n",
            "Epoch 122/150\n",
            "9083/9083 [==============================] - 2s 251us/step - loss: 9.4376 - mae: 1.6949\n",
            "Epoch 123/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 9.0636 - mae: 1.6545\n",
            "Epoch 124/150\n",
            "9083/9083 [==============================] - 2s 249us/step - loss: 9.7152 - mae: 1.7115\n",
            "Epoch 125/150\n",
            "9083/9083 [==============================] - 2s 253us/step - loss: 9.2235 - mae: 1.6814\n",
            "Epoch 126/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 8.8266 - mae: 1.6513\n",
            "Epoch 127/150\n",
            "9083/9083 [==============================] - 2s 249us/step - loss: 9.5819 - mae: 1.6905\n",
            "Epoch 128/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 8.6315 - mae: 1.6375\n",
            "Epoch 129/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 8.9010 - mae: 1.6563\n",
            "Epoch 130/150\n",
            "9083/9083 [==============================] - 2s 254us/step - loss: 9.5162 - mae: 1.6789\n",
            "Epoch 131/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.5193 - mae: 1.6940\n",
            "Epoch 132/150\n",
            "9083/9083 [==============================] - 2s 251us/step - loss: 8.7225 - mae: 1.6424\n",
            "Epoch 133/150\n",
            "9083/9083 [==============================] - 2s 249us/step - loss: 9.4892 - mae: 1.6825\n",
            "Epoch 134/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.6387 - mae: 1.6821\n",
            "Epoch 135/150\n",
            "9083/9083 [==============================] - 2s 250us/step - loss: 9.3664 - mae: 1.6885\n",
            "Epoch 136/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 8.8636 - mae: 1.6503\n",
            "Epoch 137/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 8.7639 - mae: 1.6458\n",
            "Epoch 138/150\n",
            "9083/9083 [==============================] - 2s 249us/step - loss: 8.8510 - mae: 1.6624\n",
            "Epoch 139/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 9.0255 - mae: 1.6643\n",
            "Epoch 140/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 9.0111 - mae: 1.6599\n",
            "Epoch 141/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 8.7549 - mae: 1.6582\n",
            "Epoch 142/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 8.8966 - mae: 1.6579\n",
            "Epoch 143/150\n",
            "9083/9083 [==============================] - 2s 245us/step - loss: 8.1185 - mae: 1.6062\n",
            "Epoch 144/150\n",
            "9083/9083 [==============================] - 2s 248us/step - loss: 8.8771 - mae: 1.6406\n",
            "Epoch 145/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 8.6290 - mae: 1.6355\n",
            "Epoch 146/150\n",
            "9083/9083 [==============================] - 2s 240us/step - loss: 8.6525 - mae: 1.6630\n",
            "Epoch 147/150\n",
            "9083/9083 [==============================] - 2s 241us/step - loss: 8.8022 - mae: 1.6554\n",
            "Epoch 148/150\n",
            "9083/9083 [==============================] - 2s 243us/step - loss: 8.0850 - mae: 1.6263\n",
            "Epoch 149/150\n",
            "9083/9083 [==============================] - 2s 247us/step - loss: 9.2216 - mae: 1.6620\n",
            "Epoch 150/150\n",
            "9083/9083 [==============================] - 2s 244us/step - loss: 8.5689 - mae: 1.6352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2fed8b52b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg_liWqAoy6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4c73baae-cfe9-4776-834c-6d2aba34ab9b"
      },
      "source": [
        "lstm_model.save('final_lstm.h5')\n",
        "y_pred = lstm_model.predict(testing_vectors)\n",
        "y_pred = np.around(y_pred)\n",
        "y_pred "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.],\n",
              "       [1.],\n",
              "       [9.],\n",
              "       ...,\n",
              "       [2.],\n",
              "       [8.],\n",
              "       [3.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5RnQVCNwlUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd7d8cc2-2a4f-41c4-d663-4ccfaacc1f92"
      },
      "source": [
        "lstm_model.evaluate(testing_vectors,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3893/3893 [==============================] - 0s 96us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, nan]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA9FLzIys1yH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a378e0b-bd98-45e2-8467-78db12122f27"
      },
      "source": [
        "!pip install adaptnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adaptnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/eb/9bdafff0b245cd6ffaa06550ac64fc85a558fed1bbf8b821862e6044da1f/adaptnlp-0.1.6-py3-none-any.whl (72kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (7.1.2)\n",
            "Collecting jupyterlab\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/1b/3defee93c6057a50396b2e307db3b6531b09b87a77b2e7b72ca428a963ef/jupyterlab-2.1.5-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 5.3MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 18kB/s \n",
            "\u001b[?25hCollecting flair==0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/29/81e3c9a829ec50857c23d82560941625f6b42ce76ee7c56ea9529e959d18/flair-0.4.5-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 26.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from adaptnlp) (1.0.0)\n",
            "Collecting html-text\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/8d/e43d6dcec7432732ac1955b73dcccb536d09211af343de0f1c8eb892df45/html_text-0.5.1-py2.py3-none-any.whl\n",
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (5.2.2)\n",
            "Collecting jupyterlab-server<2.0,>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7b/08d35a18a6aa02203b9c58a370fdc82619d312039394cbc5ccd1434dff3f/jupyterlab_server-1.1.5-py3-none-any.whl\n",
            "Requirement already satisfied: tornado!=6.0.0,!=6.0.1,!=6.0.2 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (4.5.3)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp) (2.11.2)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.22.2.post1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (2019.12.20)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (1.24.3)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f3/0a83558da436a081344aa6c8b85ea5b5f05071214106036ce341b7769b0b/pytest-5.4.3-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (4.41.1)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (2.8.1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (0.8.7)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.5->adaptnlp) (3.6.0)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp) (4.7.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from html-text->adaptnlp) (4.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (1.18.5)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (1.14.9)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->adaptnlp) (2.23.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (4.3.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (0.8.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (5.3.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.3.1->jupyterlab->adaptnlp) (5.0.7)\n",
            "Collecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/81/22bf51a5bc60dde18bb6164fd597f18ee683de8670e141364d9c432dd3cf/json5-0.9.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10->jupyterlab->adaptnlp) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair==0.4.5->adaptnlp) (1.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.5->adaptnlp) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5->adaptnlp) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.4.5->adaptnlp) (1.4.1)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (20.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (19.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (1.6.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (1.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (0.2.5)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair==0.4.5->adaptnlp) (8.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.5->adaptnlp) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (3.10.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.5->adaptnlp) (2.4)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.5->adaptnlp) (2.0.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp) (1.0.18)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (3.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp) (0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->adaptnlp) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp) (19.0.1)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (1.17.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0->adaptnlp) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0->adaptnlp) (2020.6.20)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.3.1->jupyterlab->adaptnlp) (4.4.2)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook>=4.3.1->jupyterlab->adaptnlp) (0.6.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.0->jupyterlab->adaptnlp) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.0->jupyterlab->adaptnlp) (47.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair==0.4.5->adaptnlp) (3.1.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.5->adaptnlp) (2.49.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->adaptnlp) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->adaptnlp) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->adaptnlp) (4.8.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->adaptnlp) (0.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->transformers==2.8.0->adaptnlp) (0.15.2)\n",
            "Building wheels for collected packages: langdetect, sqlitedict, segtok, mpld3, sacremoses\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=173495686a997a77e42a5aa5791652954f42c9b29a793b8887bab98251d2d9b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=9ff3d0ce6867e850e728299c387932257e9ddb149ad7fe1424f09a065d054aca\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=2d753a86c0680a968ec0870aab2d40b7aa0ac8f76fde10f712ea0f96058bf4a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=c1e893b69b964e6c36f445e9078dd7ecee71bbced084a09c1a9c2b8c9627832c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3645ee1c17464ec106580d9e3dfe3c1df79c414071e5dedcf2e3accb9815dbe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built langdetect sqlitedict segtok mpld3 sacremoses\n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonschema, json5, jupyterlab-server, jupyterlab, torch, langdetect, tokenizers, sacremoses, sentencepiece, transformers, sqlitedict, bpemb, pluggy, pytest, deprecated, segtok, mpld3, flair, html-text, adaptnlp\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed adaptnlp-0.1.6 bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 html-text-0.5.1 json5-0.9.5 jsonschema-3.2.0 jupyterlab-2.1.5 jupyterlab-server-1.1.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.3 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.6.0 tokenizers-0.5.2 torch-1.4.0 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P26opvzutv5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from adaptnlp import EasyWordEmbeddings, EasyStackedEmbeddings, EasyDocumentEmbeddings\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAYj3NuUGezA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f37644d-4171-4962-b988-56dc3ec36534"
      },
      "source": [
        "# Instantiate with variable number of language models\n",
        "embeddings = EasyDocumentEmbeddings(\"bert-base-cased\", \"xlnet-base-cased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "May need a couple moments to instantiate...\n",
            "Pooled embedding loaded\n",
            "RNN embeddings loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgSGtVv2ZeWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_text=filter_data['essay'].tolist()\n",
        "def semantic(i):\n",
        "  sentences = embeddings.embed_pool(example_text[i])\n",
        "  # Get the text/document embedding\n",
        "  for sentence in sentences:\n",
        "    a=sentence.get_embedding()\n",
        "    a=a.detach().numpy()\n",
        "    mean=a.mean()\n",
        "    #display(a)\n",
        "    #dff=pd.DataFrame({'semantic':mean}, index=(i for i in range(1)))\n",
        "    #isplay(dff)\n",
        "    \n",
        "for i in range(1):\n",
        "  semantic(i)\n",
        "  k=[]\n",
        "  k.append(mean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fhO0wqHspco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b581589c-24b3-4c40-d2d5-5e94b5852da0"
      },
      "source": [
        "display(k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[-0.015247249]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlPcrbzoXN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35d72ab3-c3bc-4eb1-e965-9477ca37df01"
      },
      "source": [
        "import keras\n",
        "keras.models.load_model(\"final_lstm.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f2fee57e358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    }
  ]
}